{
  "hash": "a09e4654a2c560875300ffe442ff7bc1",
  "result": {
    "markdown": "---\ntitle: \"Lab 6\"\nsubtitle: \"Hypothesis Testing\"\nauthor: |\n        | **Name:** Your name here\n        | **Mac ID:** Your Mac ID here\ndate: \"**Due:** Friday, March 3, 5 PM\"\noutput: \n  pdf_document:\n    highlight: espresso\n    fig_caption: yes\nurlcolor: blue\nheader-includes:\n    - \\usepackage{setspace}\n    - \\doublespacing\n    - \\usepackage{float}\n    - \\floatplacement{figure}{t}\n    - \\floatplacement{table}{t}\n    - \\usepackage{flafter}\n    - \\usepackage[T1]{fontenc}\n    - \\usepackage[utf8]{inputenc}\n    - \\usepackage{ragged2e}\n    - \\usepackage{booktabs}\n    - \\usepackage{amsmath}\nfontsize: 12pt\n---\n\n\n\n\n# Hypothesis Testing\n\nThe following code simulates a two-arm experiment in which the policy in question has an effect\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-1_772bcea3ecdb330efa9adaeeba6b6d2b'}\n\n```{.r .cell-code}\nN = 300 # sample size\n\ntau = 0.2 # true unobserved treatment effect\n\n# M\nmodel = declare_model(\n  N = N,\n  U = rnorm(N),\n  potential_outcomes(Y ~ tau * Z + U)\n)\n\n# I\ninquiry = declare_inquiry(\n  ATE = mean(Y_Z_1 - Y_Z_0)\n)\n\n# D\nassign = declare_assignment(\n  Z = complete_ra(N, prob = 0.5)\n  )\n\nmeasure = declare_measurement(\n  Y = reveal_outcomes(Y ~ Z)\n  )\n\n# A\nestimator = declare_estimator(\n  Y ~ Z, inquiry = \"ATE\"\n  )\n\nrct = model + inquiry + assign + measure + estimator\n```\n:::\n\n\nThe following code draws data data from a single realization of the experiment:\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-2_32816b90318fcb5b42138df1c2f12098'}\n\n```{.r .cell-code}\nset.seed(123) # replace with student id\n\nobservation = simulate_design(rct, sims = 1)\n\nobservation$estimate\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1604132\n```\n:::\n:::\n\n\nIs this estimate enough evidence to claim that the policy works? To know that we need to repeat an exercise similar to the lady tasting tea experiment we discussed in class.\n\nHowever, we now have 300 observations, so we cannot simply list all the possible ways in which the experiment could be conducted. Instead, we will simulate the same experiment in a context where the policy does not work a large number of times.\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-3_2d1d72be20d5829dcf1207cf9b93f40d'}\n\n```{.r .cell-code}\n# Redesign to make true effect 0\nno_effect = redesign(rct, tau = 0)\n\n# Simulate 1k times\nsimulation = simulate_design(no_effect, sims = 1000)\n```\n:::\n\n\nNow we can compare our `observation` with the `simulation`. The following code makes a figure with the distribution of effects when the **null hypothesis of no effect** is true and compares it with the observed effect.\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-4_9ec2c52d3b14ac2b2fe324e0d52d7f0d'}\n\n```{.r .cell-code}\nggplot(simulation) +\n  aes(x = estimate) +\n  geom_density(linewidth = 2) + \n  geom_vline(xintercept = observation$estimate,\n             linetype = \"dashed\",\n             linewidth = 2,\n             color = \"red\") +\n  annotate(\"text\", \n           x =  observation$estimate + 0.1,\n           y = 0,\n           label = \"Observed effect\",\n           size = 4,\n           color = \"red\") +\n  labs(x = \"Simulated effects when policy has no effect\",\n       y = \"Density\")\n```\n\n::: {.cell-output-display}\n![](08_lab_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe visual illustration is helpful, but perhaps we want a one-number summary of the probability of observing something equal or more extreme than what we observed when the policy has no effect.\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-5_01d6e8f0fbd15a9015e29cdfee1d6064'}\n\n```{.r .cell-code}\n# Proportion of simulated estimates\n# more extreme than the observed estimate\n# either more positive or more negative\nmean(abs(simulation$estimate) >= abs(observation$estimate))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.167\n```\n:::\n:::\n\n\nThis is our **p-value**, the proportion of simulated estimates that are equal or more extreme than what we observed.\n\nFortunately, we do not need to calculate this by hand for most research designs. The `DeclareDesign` package takes care of that and we already stored the p-value when we saved `observation`.\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-6_ce0c97ced5bf8d7031987946ddad1f9d'}\n\n```{.r .cell-code}\nobservation$p.value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1435053\n```\n:::\n:::\n\n\nYou may get slightly different numbers when you calculate p-values by hand and use the one generated by `DeclareDesign`. This is normal, most packages use an off-the-shelf formula to calculate p-values to approximate simulated p-values while saving computing time.\n\n## TASK 1\n\n**In the previous exercise , how do you interpret** `observation$estimate` **in words?**\n\n**How do you interpret** `observation$p-value` **in words?**\n\n**Based on these two pieces of information, would you feel confident claiming that the policy works? Why?**\n\n\n# Statistical Power\n\nExperiments can always give small p-values by chance, even if the policy has no effect. If our p-value gives a lot of evidence against the null hypothesis of no effect but there is no effect in reality, we call this a **false positive**. Our answer and data strategies suggest there is an effect when there is none.\n\nHow can we minimize the possibility of false positives **before** conducting an experiment? Our research design should meet two criteria:\n\n1. The chances of false positives are low\n2. The chances of detecting a true effect when it exists are high\n\nWe meet the first criterion by requiring p-values to be very small before claiming we have enough evidence against the null hypothesis. A common number in the social sciences is $p < 0.05$. This number made sense when researchers needed to calculate p-values by hand. It does not make much sense nowadays, but it's still a useful rule of thumb for most applications.\n\nFor the second criterion, we can calculate the **statistical power** of an experimental answer strategy by simulating many realizations of the same experiment, assuming there is an effect, and calculating how often p-values surpass an arbitrary threshold.\n\nFor example, the following simulates many realizations of the `rct` design and calculates power under $p < 0.05$.\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-7_f65babf9e7b9f6de5b59817fb1af0f87'}\n\n```{.r .cell-code}\nset.seed(123) # replace with student id\n# simulate design with effect many times\nrct_sim = simulate_design(rct, sims = 1000)\n\n# calculate proportion of p.values smaller than 0.05\npower = mean(rct_sim$p.value < 0.05)\n\npower\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.39\n```\n:::\n:::\n\n\nThis proportion represents how often we would be willing to claim there is enough evidence against the null hypothesis of no effect. This is for a research design for which **we already know there is an effect** because we declared it so in our `model` step.\n\nOnce again, this is not something that you need to calculate by hand. You obtain this when you diagnose a research design.\n\n\n::: {.cell hash='08_lab_cache/html/unnamed-chunk-8_e02a3c11f434eb3367c015dbab5b061f'}\n\n```{.r .cell-code}\nset.seed(123) # replace with student id\n\ndiag = diagnose_design(rct)\n\ndiag\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nResearch design diagnosis based on 500 simulations. Diagnosis completed in 10 secs. Diagnosand estimates with bootstrapped standard errors in parentheses (100 replicates).\n\n Design Inquiry Estimator Outcome Term N Sims Mean Estimand Mean Estimate\n    rct     ATE estimator       Y    Z    500          0.20          0.20\n                                                     (0.00)        (0.01)\n   Bias SD Estimate   RMSE  Power Coverage\n   0.00        0.11   0.11   0.40     0.95\n (0.01)      (0.00) (0.00) (0.02)   (0.01)\n```\n:::\n:::\n\n\n\nThis output has a column named `power` that should be similar to how we calculate it by hand, discounting simulation variability and rounding.\n\n## TASK 2\n\n**Based on the statistical power that you saw above, would you say this is a good research design to evaluate the policy in question?**\n\n**What happens to the power of the** `rct` **design when you increase** `tau` **? Why?**\n\n**What happens to power when you make** `tau` **equal zero? Why?**\n\n**What happens to power when you increase or decrease (pick one)** `N` **? Why?**\n\n# Answers\n\nWrite your answers here. Remember to show the code you use to get the answer!\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
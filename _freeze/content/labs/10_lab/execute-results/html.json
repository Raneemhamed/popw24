{
  "hash": "78096b4812257aa0155f89b6cb8abfb9",
  "result": {
    "markdown": "---\ntitle: \"Lab 8\"\nsubtitle: \"Field experimental designs II\"\nauthor: |\n        | **Name:** Your name here\n        | **Mac ID:** Your Mac ID here\ndate: \"**Due:** Friday, March 17, 5 PM\"\noutput: \n  pdf_document:\n    highlight: espresso\n    fig_caption: yes\nurlcolor: blue\nheader-includes:\n    - \\usepackage{setspace}\n    - \\doublespacing\n    - \\usepackage{float}\n    - \\floatplacement{figure}{t}\n    - \\floatplacement{table}{t}\n    - \\usepackage{flafter}\n    - \\usepackage[T1]{fontenc}\n    - \\usepackage[utf8]{inputenc}\n    - \\usepackage{ragged2e}\n    - \\usepackage{booktabs}\n    - \\usepackage{amsmath}\nfontsize: 12pt\n---\n\n\n\n\n# Pre-post design\n\n[Diaz and Rossiter (2023)](https://gustavodiaz.org/files/research/precision_retention.pdf) argue that, while implementing pre-post designs is generally a good idea, doing so sometimes entails sacrificing some of your sample size, which may offset the precision gains.\n\nTo illustrate this point, let's simulate a design that compares the standard design and the pre-post design, but implementing the pre-post design means losing a proportion of the observations.\n\nThe following code does this, but notice that we are missing the step of putting all parts of the model together.\n\n\n::: {.cell hash='10_lab_cache/html/unnamed-chunk-1_5d81f792f2ad48d0992b07293befa1c3'}\n\n```{.r .cell-code}\ndrop = 0.2 # proportion dropped in pre-post design\n\n# M\npp_model = declare_model(\n  N = 500,\n  U = rnorm(N)/2,\n  # time-specific unobserved factors\n  e1 = rnorm(N),\n  e2 = rnorm(N),\n  # R indicates if unit stays in sample,\n  R = rbinom(N, 1, prob  = 1 - drop),\n  Y1 = U + e1, # outcome before treatment,\n  potential_outcomes(Y2 ~ 0.3*Z + Y1 + e2)\n)\n\n# I\npp_inq = declare_inquiry(\n  ate = mean(Y2_Z_1 - Y2_Z_0)\n)\n\n# D\npp_assign = declare_assignment(\n  Z = complete_ra(N)\n)\n\npp_measure = declare_measurement(\n  Y2 = reveal_outcomes(Y2 ~ Z)\n)\n\n# A\n## estimate standard experiment\n## with regression to make\n## comparison fair\npp_standard = declare_estimator(\n  Y2 ~ Z,\n  .method = lm_robust,\n  inquiry = \"ate\",\n  label = \"Standard\"\n)\n\npp_prepost = declare_estimator(\n  Y2 ~ Z + Y1,\n  .method = lm_robust,\n  subset = R == 1,\n  inquiry = \"ate\",\n  label = \"Pre-post\"\n)\n```\n:::\n\n\n## TASK 1\n\n**Compare the performance of the standard experiment and the pre-post experiment in terms of bias, RMSE. Which design would you recommend and why?**\n\n**Does your recommendation change if the pre-post experiment now sacrifices half of the sample? Explain**\n\n**Does your recommendation change if the pre-post experiment now loses more than half of the sample? Explain**\n\n# Block randomization\n\nA famous textbook on field experiments suggests that the biggest downside of conducting a block-randomized experiment is inadvertently analyzing them as if they assigned conditions through simple or complete randomization [(Gerber and Green 2012)](https://wwnorton.com/books/9780393979954).\n\nHow bad could this problem be? The following code simulates a block-randomized experiment and compares two answer strategies: A **naive** estimator and a **block** estimator. Once again, we are skipping the step of combining the different elements of the design into one.\n\n\n::: {.cell hash='10_lab_cache/html/unnamed-chunk-2_0b580d76611373c791c10affb88ce893'}\n\n```{.r .cell-code}\n# True ATEs per block\ntau = c(4, 2, 0)\n\n# M\nb_model = declare_model(\n  block = add_level(N = 3,\n                    prob = c(0.5, 0.7, 0.9),\n                    tau = tau),\n  indiv = add_level(N = 100, \n                    U = rnorm(N),\n                    Y_Z_0 = U,\n                    Y_Z_1 = U + tau)\n)\n\n# I\n\nb_inq = declare_inquiry(\n  ate = mean(Y_Z_1 - Y_Z_0)\n)\n\n# D\nb_assign = declare_assignment(\n  Z = block_ra(blocks = block,\n               block_prob = c(.5, .7, .9)),\n  Z_cond_prob = obtain_condition_probabilities(\n    assignment = Z, \n    blocks = block, \n    block_prob = c(.5, .7, .9))\n)\n  \nb_measure = declare_measurement(\n  Y = reveal_outcomes(Y ~ Z)\n)\n\n# A\nb_naive = declare_estimator(\n  Y ~ Z,\n  .method = difference_in_means,\n  inquiry = \"ate\",\n  label = \"Naive\"\n)\n\nb_block = declare_estimator(\n  Y ~ Z,\n  blocks = block,\n  .method = difference_in_means,\n  inquiry = \"ate\",\n  label = \"Block\"\n)\n```\n:::\n\n\n## TASK 2\n\n**Compare both answer strategies in terms of bias, RMSE, and power. Which one would you recommend and why?**\n\n**What happens if you pick three new values for the true ATEs per block stored in** `tau`**? Explain**\n\n**What happens if the three ATEs per block have the same value? Explain**\n\n# Answers\n\nWrite your answers here. Remember to show the code you use to get the answer and to use `set.seed` with your student number to get unique results in simulations and diagnosis.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
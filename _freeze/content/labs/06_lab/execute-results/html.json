{
  "hash": "e9833d6a0fd7fa50699af8c069bfd4c5",
  "result": {
    "markdown": "---\ntitle: \"Lab 5\"\nsubtitle: \"External Validity\"\nauthor: |\n        | **Name:** Your name here\n        | **Mac ID:** Your Mac ID here\ndate: \"**Due:** Friday, February 17, 5 PM\"\noutput: \n  pdf_document:\n    highlight: espresso\n    fig_caption: yes\nurlcolor: blue\nheader-includes:\n    - \\usepackage{setspace}\n    - \\doublespacing\n    - \\usepackage{float}\n    - \\floatplacement{figure}{t}\n    - \\floatplacement{table}{t}\n    - \\usepackage{flafter}\n    - \\usepackage[T1]{fontenc}\n    - \\usepackage[utf8]{inputenc}\n    - \\usepackage{ragged2e}\n    - \\usepackage{booktabs}\n    - \\usepackage{amsmath}\nfontsize: 12pt\n---\n\n\n\n\n# External Validity\n\nIn the readings for this week, [Coppock et al (2018)](https://doi.org/10.1073/pnas.1808083115) mention how the correspondence in effects between representative and convenience samples depends on the distribution of individual treatment effects.\n\nThe following design simulates a model with heterogeneous treatment effects, and compares the result of survey experiments conducted with a representative and convenience sample.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Parameters\nN = 1000 # population\n\nn = 100 # sample\n\neffect = 0.5\n\n# Model\nmodel = declare_model(\n  N = N,\n  U = rnorm(N),\n  X = runif(N), # observed covariate\n  potential_outcomes(\n    Y ~ Z * effect * X + U\n    )\n)\n```\n:::\n\n\nWe are also specifying `X` as an observed covariate that moderates the treatment effect, something like digital literacy. It's generated by random draws of a uniform distribution between 0 and 1 (hence the `runif` function). If `X` is 1, the unit experiences the full effect. If it is zero, the effect disappears. The numbers in between scale the treatment effect accordingly. This is a way to simulate heterogeneous treatment effects.\n\nThe inquiry is standard fare:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inquiry\ninquiry = declare_inquiry(\n  ATE = mean(Y_Z_1 - Y_Z_0)\n)\n```\n:::\n\n\nThen we have to compare two data strategies, the survey experiment with a random sample and the one with a convenience sample. At this point our research design branches into two paths, since each data strategy will also have its own analogous answer strategy. They are essentially two different designs, but we can recycle some components.\n\nThis is how it looks for the representative sample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data strategy\nr_sampling = declare_sampling(S = complete_rs(N, n = n))\n\nassignment = declare_assignment(Z = complete_ra(N))\n\n# Answer strategy\nmeasurement = declare_measurement(Y = reveal_outcomes(Y ~ Z))\n\nestimator = declare_estimator(\n  Y ~ Z, \n  inquiry = \"ATE\"\n  )\n```\n:::\n\n\n\nThen put everything together:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr_design = model + inquiry +\n  r_sampling + assignment +\n  measurement + estimator\n```\n:::\n\n\nTo create our convenience sample, we need a custom sampling function that makes it so that units with higher `X` are more likely to be drawn.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nconvenience_sampling = function(data){\n  id = sample(data$ID, size = n, prob = data$X) \n  \n  data$S = with(\n    data,\n    ifelse(\n      data$ID %in% id, 1, 0\n      )\n    )\n  \n  data[data$S == 1, ]\n}\n```\n:::\n\n\nThen we pass this custom function to `declare_sampling`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc_sampling = declare_sampling(handler = convenience_sampling)\n```\n:::\n\n\n\nAnd now we can create a separate design for our convenience sample\n\n\n::: {.cell}\n\n```{.r .cell-code}\nc_design = model + inquiry +\n  c_sampling + assignment +\n  measurement + estimator\n```\n:::\n\n\nThen we can diagnose both designs at once:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remember to replace with student number\nset.seed(123) \n\nr_diag = diagnose_design(r_design)\n\nc_diag = diagnose_design(c_design)\n```\n:::\n\n\nAnd we can use the following function to fetch the bias and RMSE of each design.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagnosands = rbind(\n  r_diag$diagnosands_df %>%\n  select(design, bias, rmse),\n  c_diag$diagnosands_df %>%\n  select(design, bias, rmse)\n)\n\ndiagnosands\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    design         bias      rmse\n1 r_design -0.003418615 0.2066810\n2 c_design  0.067789877 0.2099169\n```\n:::\n:::\n\n\n## TASK 1\n\n**Which design is better in terms of bias and RMSE? What explains this?**\n\n## TASK 2\n\n**What happens to the bias and RMSE of both designs as the sample size** `n` **decreases but the population** `N` **remains constant? What explains this?** \n\n## TASK 3\n\n**What happens to the bias and RMSE when the population and sample sizes are the same? What explains this?**\n\n**Hint:** *It may be faster to calculate this by choosing a number in between the original population and sample sizes.*\n\n# Answers\n\n## TASK 1\n\n## TASK 2\n\n## TASK 3\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "478faf3a40763781e6d0a72023073c3d",
  "result": {
    "markdown": "---\ntitle: \"Lab 3\"\nsubtitle: \"Reducing sensitivity bias\"\nauthor: |\n        | **Name:** Your name here\n        | **Mac ID:** The first half of your Mac email address\ndate: \"**Due:** Friday, February 3, 5 PM\"\noutput: \n  pdf_document:\n    highlight: espresso\n    fig_caption: yes\nurlcolor: blue\nheader-includes:\n    - \\usepackage{setspace}\n    - \\doublespacing\n    - \\usepackage{float}\n    - \\floatplacement{figure}{t}\n    - \\floatplacement{table}{t}\n    - \\usepackage{flafter}\n    - \\usepackage[T1]{fontenc}\n    - \\usepackage[utf8]{inputenc}\n    - \\usepackage{ragged2e}\n    - \\usepackage{booktabs}\n    - \\usepackage{amsmath}\nfontsize: 12pt\n---\n\n\n\n\n# Part 1: Bias-variance tradeoff in list experiments\n\nThis week, we learned about techniques to prevent respondents from concealing their answer to sensitive questions. We saw how these research designs reduce sensitivity bias, but this bias-reduction is not free.\n\nA golden rule in statistics is **bias-variance** tradeoff. **Bias** is similar to validity, an estimator is unbiased if it produces estimates that average to the true, unobserved value of the quantity of interest (the *estimand*) over repeated hypothetical realizations of the same event generation process.\n\nAn estimator is **precise** or has **low variance** when it produces estimates that are close to each other over repeated hypothetical realizations of the same event generation process.\n\nThis figure illustrates how estimators can be biased, precise, both, or neither. It is the same figure we saw for validity and reliability: <https://popw23.gustavodiaz.org/slides/figs/darts_bias.png>\n\nThe issue is that you can rarely improve upon one of the two dimensions without sacrificing the other. In the context of research designs to reduce sensitivity bias, direct questions are biased, but because they are straightforward they are precise.\n\nOn the flip side, research designs to reduce sensitivity bias do so by introducing noise in the measurement, which makes the resulting estimates more imprecise.\n\nWe can illustrate this with the list experiment. The following code simulates a list experiment and compares it to a direct question. We start with our model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nN = 500\n\n#  Model\nlist_model = declare_model(\n  N = N, \n  control_count = rbinom(N, size = 3, prob = 0.5), \n  sensitive = rbinom(N, size = 1, prob = 0.3),\n  lie = case_when(sensitive == 0 ~ 0L,\n                  sensitive == 1 ~ rbinom(N, size = 1, prob = 0.2)),\n  potential_outcomes(Y_list ~ sensitive * Z + control_count)\n)\n```\n:::\n\n\nOur model considers a survey of `N = 500` respondents. In this case, we are agnostic about where the sample came from. `control_count` generates 3 baseline items, each of them applying to a respondent with probability 0.5. In turn, any given respondent holds the `sensitive` trait of interest with probability 0.3. Which will roughly approximate to a prevalence rate of about 30%. You may want to spend some time trying to understand what the `rbinom` function does.\n\nThe `lie` part looks weird, but it's just saying that if a person holds the sensitive trait ($\\text{sensitive} = 1$), then they conceal the truth if asked directly with probability 0.2 If $\\text{sensitive} = 0$, they simply tell the truth since there is nothing to conceal. `case_when` is a very interesting function that you may want to learn about.\n\nThe `potential_outcomes` step is also new. We are using it to declare how people would respond to being assigned to the treatment or control group. As you will see later, `Z` will take a value of 1 under treatment and 0 under control. If $Z = 1$, the true response to the \"how many\" question will be `sensitive + control_count`. If $Z = 0$, the response will be just `control_count`. We will talk more about the potential outcomes framework next week.\n\nThe next step is the **inquiry**, which in this case is the proportion of people who hold the sensitive trait.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Inquiry\nlist_inquiry = declare_inquiry(prevalence_rate = mean(sensitive))\n```\n:::\n\n\nThen we declare our data strategy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nlist_assignment = declare_assignment(Z = complete_ra(N))\n\nlist_measurement = declare_measurement(Y_list = reveal_outcomes(Y_list ~ Z),\n                                       Y_direct = sensitive - lie)\n```\n:::\n\n\nWhich in this case has two parts, first we indicate that condition `Z` in the list experiment is assigned by complete random assignment. Then we realize outcomes for responses to both the list experiment and direct questions.\n\nFinally, we need answer strategies for each of our outcomes. We have two estimators for the same inquiry.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Answer\nlist_estimator = declare_estimator(Y_list ~ Z, \n                                   inquiry = \"prevalence_rate\",\n                                   label = \"list\")\n\ndirect_estimator = declare_estimator(Y_direct ~ 1, \n                                     inquiry = \"prevalence_rate\",\n                                     label = \"direct\")\n```\n:::\n\n\nWe can now put everything together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_design = list_model + list_inquiry + list_assignment +\n  list_measurement + list_estimator + direct_estimator\n```\n:::\n\n\nIf you are curious, you can use the `draw_data` function to see how one realization of our design looks like. But make sure you do not print this output to the console! I included a code chunk option to prevent this.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndraw_data(list_design)\n```\n:::\n\n\nOne more thing we have to do is define standards to evaluate the bias and precision of our estimators.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlist_diagnosands = declare_diagnosands(\n  bias = mean(estimand - estimate),\n  RMSE = sqrt(mean((estimand - estimate)^2))\n)\n```\n:::\n\n\nWe are using one of many ways to measure bias. Our measure of precision is the Root Mean Squared Error, which is similar to how we measure bias but the squared part penalizes estimates that deviate too much from the estimand.\n\nLet's use the `diagnose_design` function to compare the list experiment with direct responses.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234) # replace with your student ID\n\nlist_diag = diagnose_design(list_design, diagnosands = list_diagnosands)\n\n# Inspect the object but hide output in the pdf\nlist_diag\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n  estimator   bias  RMSE\n1    direct  0.060 0.061\n2      list -0.004 0.084\n```\n:::\n:::\n\n\nSo you can see that the direct question has more bias than the list experiment. This is because in our model some participants conceal the truth under the direct question. The list experiment has essentially zero bias because our model assumes that the list experiment works.\n\nHowever, you can also see that the direct question has lower RMSE than the list experiment, which means that it is more precise. This suggests that using a list experiment will produce estimates that vary more than the direct question over multiple hypothetical realizations of the event generation process.\n\nThis implies that we may not be able to convince our audience that the list experiment performs any better than asking questions directly. We may get unlucky and get an estimate that is even further from the truth.\n\n## TASK 1\n\n**Can the list experiment perform better in terms of RMSE if we increase the sample size? Repeat the research design above using different values of $N$ until you find the smallest possible sample size under which the RMSE of the list experiment is smaller than that of the direct question.**\n\nSome tips:\n\n- There are multiple ways to accomplish this. You can try multiple values of $N$ at once or do one at a time. In either case, using the `redesign` function may save you some time.\n\n- If you end up doing one value of $N$ at a time, you only need to report the value that you end up sticking with, but you need to explain with words the procedure you used to come up with that number.\n\n# Part 2: Randomized and non-randomized response designs\n\n## Randomized response\n\nThe following code simulates a randomized-response research design and compares it to a direct question. Respondents will answer *yes* if the dice comes 1, 2, 3, or 4, and will answer truthfully if the dice shows 5 or 6.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Model\nrr_model = declare_model(\n  N = 1000,\n  sensitive = rbinom(N, size = 1, prob = 0.1),\n  lie = case_when(sensitive == 0 ~ 0L,\n                  sensitive == 1 ~ rbinom(N, size = 1, prob = 0.5)),\n  direct = sensitive - lie\n)\n\n# In this case we need to declare potential outcomes as\n# a separate step.\n# Say yes if forced to, otherwise tell truth\nrr_pot = declare_potential_outcomes(Y_Z_yes = 1, Y_Z_truth = sensitive)\n\n## Inquiry\nrr_inquiry = declare_inquiry(prevalence_rate = mean(sensitive))\n\n## Data\n# Forced to say yes with probability 4/6\nrr_assignment = declare_assignment(Z = complete_ra(N, prob = 4/6, \n    conditions = c(\"truth\", \"yes\")))\n\nrr_measurement = declare_reveal(Y, Z)\n```\n:::\n\n\nFor the answer strategy, we will need to create custom estimator functions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# RR estimator\nrr = function(data){\n  with(data, \n    data.frame(estimate = (mean(Y) - 4/6)/(1 - 4/6)))\n}\n\n# Direct question estimator\ndirect = function(data){\n  with(data, \n    data.frame(estimate = mean(direct)))\n}\n```\n:::\n\n\nAnd then we can encode these as custom estimators in `DeclareDesign`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## Answer\nrr_estimator = declare_estimator(handler = label_estimator(rr),\n                                 inquiry = \"prevalence_rate\",\n                                 label = \"Randomized response\")\n\nrr_direct = declare_estimator(handler = label_estimator(direct),\n                              inquiry = \"prevalence_rate\",\n                              label = \"Direct question\")\n```\n:::\n\n\nNow we can put everything together\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr_design = rr_model + rr_pot +rr_inquiry + rr_assignment + \n  rr_measurement + rr_estimator + rr_direct\n```\n:::\n\n\nAnd then diagnose:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrr_diag = diagnose_design(rr_design)\n\n# Inspect but hide output in pdf\nrr_diag\n```\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\n            estimator  bias  rmse\n1     Direct question -0.05 0.050\n2 Randomized response  0.00 0.013\n```\n:::\n:::\n\n\n## Crosswise model\n\nIn one of the assigned readings, [Oliveros and Gingerich (2020)](https://doi.org/10.1093/ijpor/edz019) use a crosswise model instead of a randomized response model. This variant is a **non-randomized response** design. In their case, their vignette looks like this:\n\n> **How many of the following statements are true**\n>\n> -   My mother was born in OCTOBER, NOVEMBER, OR DECEMBER\n> -   In order to avoid paying a traffic ticket, I would be willing to pay a bribe to a police officer\n>\n> **Please indicate your answer below**\n>\n> A. **both** statements are true OR **neither** statement is true\n>\n> B. **one** of the two statements is true\n\nThis design has two advantages. First, respondents never answer to the sensitive question directly, which protects anonymity even more than the randomized response design. Second, the question does not depend on a randomization device.\n\nThis design gives us everything we need to estimate the proportion of people who hold the sensitive trait, which in this case is the willingness to bribe a police officer. Let's call that quantity of interest $\\pi$.\n\nWe need an innocuous statement that is completely unrelated to the sensitive item but that the researcher can know its probability $p$ ahead of time. The only requirement is that $p \\neq 0.5$. In this case $p = 0.265$ was found in a phone survey conducted before the present study.\n\nRespondents can only choose A or B, so we can express this as $\\lambda = 1$ for A and $\\lambda = 0$ for B.\n\nThe formula for the proportion of respondents who will select option A is\n\n$$\n\\lambda = \\text{Prob(both true) + Prob(neither true)}\n$$\n\nwe can replace with the quantities described above to get\n\n$$\n\\lambda = p\\pi + (1-p)(1-\\pi)\n$$\n\nand then solve for $\\pi$ to get the estimator $\\widehat{\\pi}$\n\n$$\n\\widehat{\\pi} = \\frac{\\lambda + p - 1}{2p-1}\n$$\n\nWhich we read as \"pi hat.\" The hat reminds us this is an estimator, meaning an approximation of the estimand or the true prevalence rate.\n\n## TASK 2\n\n**Write a research design that simulates the crosswise model as described here. How does it perform in comparison to the direct question and randomized response in terms of bias and RMSE? On statistical grounds, which of the three research designs (direct, RR, crosswise) is better for the current even generation process? Which one seems better in terms of ensuring anonymity? For the last two questions, explain why you think so.**\n\nHere are a few hints:\n\n- You will need to write a new estimator for the crosswise model\n-  The crosswise model has no random component, so there won't be a potential outcomes part\n-  You will need to encode individual responses for $p$ and $\\lambda$ in the model part\n-  You do not need to include a comparison to the randomized response nor the direct question in your research design. Among other things, this means you do not need to include `lie` in the event generation process\n-  The `case_when` function is your friend\n- Use the `draw_data` as we did above to make sure that your design is simulating data that maps the event generation process\n\nThis is the first time I ask you to write a research design from scratch. If you get stuck and are running out of time, I would still like to see your code even if it doesn't work along with an explanation of what you were hoping to accomplish. You can use the `error = TRUE` code chunk option to make sure your PDF knits even if there are errors in your code.\n\nOne last tip, work together! You will have an easier time figuring things out by talking to your peers.\n\n# Answers\n\nInclude your answers in the sub-sections below. You should show your code and its output when relevant. Avoid printing overly long objects, such as simulated data.\n\nYou should include enough prose to make sure I understand why you did what you did and how it led you to the final answer. When in doubt, it is better to err on the side of explaining more than necessary.\n\n\n\n## TASK 1\n\n\n## TASK 2\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}